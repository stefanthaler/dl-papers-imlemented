{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel Softmax\n",
    "\n",
    "\n",
    "* Pre-print, published in ICLR 2017 https://arxiv.org/pdf/1611.01144.pdf\n",
    "\n",
    "* https://arxiv.org/pdf/1406.2989.pdf\n",
    "\n",
    "\n",
    "## Experiments\n",
    "\n",
    "* **Dataset**: We use the MNIST dataset with fixed binarization for training and evaluation\n",
    "* **Tricks**: We also found that variance normalization was necessary\n",
    "* **Network**: We used sigmoid activation functions for binary (Bernoulli) neural networks and softmax activations for categorical variables.\n",
    "* **Training**: Models were trained using stochastic gradient descent with momentum 0.9.\n",
    "* **Learning rates**:  are chosen from {3e−5, 1e−5, 3e−4, 1e−4, 3e−3, 1e−3}; we select the best learning rate for each estimator using the MNIST validation set, and report performance on the test set.\n",
    "* **Tasks** Each estimator is evaluated on two tasks: (1) structured output prediction and (2) variational training of generative models. \n",
    "\n",
    "### 1) Structured output prediction with stochastic binary networks\n",
    "\n",
    "### 2) Generative modelling with variational Autoencoders\n",
    "\n",
    "### 3) Generative semi supervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset binarized_mnist/1.0.0 (download: 104.68 MiB, generated: Unknown size, total: 104.68 MiB) to /tf/data/binarized_mnist/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a16617c652428689c6c1be2de605f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecd18714f5341399e4bd296f540107b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /tf/data/binarized_mnist/1.0.0.incompleteFZRZTW/binarized_mnist-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417685be0f074657b0af511175a61344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /tf/data/binarized_mnist/1.0.0.incompleteFZRZTW/binarized_mnist-validation.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4056e8c940e3453185479b89728caa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /tf/data/binarized_mnist/1.0.0.incompleteFZRZTW/binarized_mnist-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8ec004ffe546b8b284e4d6638bf67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset binarized_mnist downloaded and prepared to /tf/data/binarized_mnist/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "mnist_data = tfds.load(\"binarized_mnist\", data_dir=\"/tf/data\")\n",
    "mnist_train, mnist_test = mnist_data[\"train\"], mnist_data[\"test\"]\n",
    "assert isinstance(mnist_train, tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split upper half / lower half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: {image: (28, 28, 1)}, types: {image: tf.uint8}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lower_upper_half(image):\n",
    "    print(image[\"image\"])\n",
    "    flat_image = tf.reshape(image[\"image\"], [-1], name=None)\n",
    "    upper_half, lower_half = tf.split(flat_image, num_or_size_splits=2, axis=0, num=None, name='split_image_to_upper_lower_half')\n",
    "\n",
    "    \n",
    "    return upper_half, lower_half#upper_half, lower_half # x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(28, 28, 1), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: (784,), types: tf.uint8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_train_ds = mnist_train.map(split_lower_upper_half, num_parallel_calls=AUTOTUNE)\n",
    "labeled_train_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel Softmax \n",
    "* https://gist.github.com/ericjang/1001afd374c2c3b7752545ce6d9ed349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20): \n",
    "    \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "    U = tf.random_uniform(shape,minval=0,maxval=1)\n",
    "    return -tf.log(-tf.log(U + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature): \n",
    "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "    y = logits + sample_gumbel(tf.shape(logits))\n",
    "    return tf.nn.softmax( y / temperature)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "    Args:\n",
    "        logits: [batch_size, n_class] unnormalized log-probs\n",
    "        temperature: non-negative scalar\n",
    "        hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "    Returns:\n",
    "        [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "        If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "        be a probabilitiy distribution that sums to 1 across classes\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    if hard:\n",
    "        k = tf.shape(logits)[-1]\n",
    "        #y_hard = tf.cast(tf.one_hot(tf.argmax(y,1),k), y.dtype)\n",
    "        y_hard = tf.cast(tf.equal(y,tf.reduce_max(y,1,keep_dims=True)),y.dtype)\n",
    "        y = tf.stop_gradient(y_hard - y) + y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Structured output prediction with stochastic binary networks\n",
    "\n",
    "* **Task**: Predict lower half of mnist image given top half.\n",
    "* The minimization objective for this conditional generative model is an importance-sampled estimate of the likelihood objective, Eh∼pθ(hi|xupper)  m Pm i=1 log pθ(xlower|hi)\n",
    "\n",
    "*  where m = 1 is used for training and m = 1000 is used for evaluation.\n",
    "\n",
    "* For bernoulli variables they use signmoid activation\n",
    "* For categorical variables they use \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmaxStructuredOutputPrediciton(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Predicts lower half of an mnist image given the top half.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GumbelSoftmaxStructuredOutputPrediciton, self).__init__()\n",
    "    \n",
    "    def setup_model(self):\n",
    "        self.input_layer = tf.keras.layers.Dense(200, activation=tf.nn.sigmoid) # [bs,392] => [bs,200]\n",
    "        self.categorical_layer = tf.keras.layers.Dense(200, activation=None) # [bs,200] => [bs,200]\n",
    "        self.output_layer = tf.keras.layers.Dense(392, activationtf.nn.sigmoid) # [bs,200] => [bs,392]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def call(self, upper_image_half, temperature):\n",
    "        h1 = self.input_layer(output_layer)\n",
    "        logits = self.categorical_layer(h1) \n",
    "        h2 = gumbel_softmax_sample(logits, temperature)\n",
    "        lower_image_half = self.output_layer(h3)       \n",
    "        return lower_image_half\n",
    "\n",
    "sop_model = GumbelSoftmaxStructuredOutputPrediciton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=False, name='SGD') # m {3e−5, 1e−5, 3e−4, 1e−4, 3e−3, 1e−3};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* They use negative log likelihood. \n",
    "\n",
    "TODO not sure of what, it's not quite obvious from the paper\n",
    "\n",
    "tf.nn.sigmoid_cross_entropy_with_logits \n",
    "\n",
    "-tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loss, test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = \n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images_upper_half, images_lower_half):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images_upper_half, training=True)\n",
    "    loss = loss_object(images_lower_half, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for images, labels in train_ds:\n",
    "    train_step(images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print(template.format(epoch + 1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result() * 100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
