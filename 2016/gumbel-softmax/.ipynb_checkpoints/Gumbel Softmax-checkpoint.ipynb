{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel Softmax\n",
    "\n",
    "\n",
    "* Pre-print, published in ICLR 2017 https://arxiv.org/pdf/1611.01144.pdf\n",
    "\n",
    "* https://arxiv.org/pdf/1406.2989.pdf\n",
    "\n",
    "\n",
    "## Experiments\n",
    "\n",
    "* **Dataset**: We use the MNIST dataset with fixed binarization for training and evaluation\n",
    "* **Tricks**: We also found that variance normalization was necessary\n",
    "* **Network**: We used sigmoid activation functions for binary (Bernoulli) neural networks and softmax activations for categorical variables.\n",
    "* **Training**: Models were trained using stochastic gradient descent with momentum 0.9.\n",
    "* **Learning rates**:  are chosen from {3e−5, 1e−5, 3e−4, 1e−4, 3e−3, 1e−3}; we select the best learning rate for each estimator using the MNIST validation set, and report performance on the test set.\n",
    "* **Tasks** Each estimator is evaluated on two tasks: (1) structured output prediction and (2) variational training of generative models. \n",
    "\n",
    "### 1) Structured output prediction with stochastic binary networks\n",
    "\n",
    "### 2) Generative modelling with variational Autoencoders\n",
    "\n",
    "### 3) Generative semi supervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # do we need that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "mnist_data = tfds.load(\"binarized_mnist\", data_dir=\"/tf/data\")\n",
    "mnist_train, mnist_test = mnist_data[\"train\"], mnist_data[\"test\"]\n",
    "assert isinstance(mnist_train, tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split upper half / lower half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: {image: (28, 28, 1)}, types: {image: tf.uint8}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lower_upper_half(image):\n",
    "    print(image[\"image\"])\n",
    "    flat_image = tf.reshape(image[\"image\"], [-1], name=None)\n",
    "    upper_half, lower_half = tf.split(flat_image, num_or_size_splits=2, axis=0, num=None, name='split_image_to_upper_lower_half')\n",
    "\n",
    "    \n",
    "    return upper_half, lower_half#upper_half, lower_half # x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(28, 28, 1), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((392,), (392,)), types: (tf.uint8, tf.uint8)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_train_ds = mnist_train.map(split_lower_upper_half, num_parallel_calls=AUTOTUNE)\n",
    "labeled_train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_ds = labeled_train_ds.shuffle(100,reshuffle_each_iteration=True).batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel Softmax \n",
    "* https://gist.github.com/ericjang/1001afd374c2c3b7752545ce6d9ed349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20): \n",
    "    \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "    U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "    return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature): \n",
    "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "    y = logits + sample_gumbel(tf.shape(logits))\n",
    "    return tf.nn.softmax( y / temperature)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "    Args:\n",
    "        logits: [batch_size, n_class] unnormalized log-probs\n",
    "        temperature: non-negative scalar\n",
    "        hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "    Returns:\n",
    "        [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "        If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "        be a probabilitiy distribution that sums to 1 across classes\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    if hard:\n",
    "        k = tf.shape(logits)[-1]\n",
    "        #y_hard = tf.cast(tf.one_hot(tf.argmax(y,1),k), y.dtype)\n",
    "        y_hard = tf.cast(tf.equal(y,tf.reduce_max(y,1,keep_dims=True)),y.dtype)\n",
    "        y = tf.stop_gradient(y_hard - y) + y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Structured output prediction with stochastic binary networks\n",
    "\n",
    "* **Task**: Predict lower half of mnist image given top half.\n",
    "* The minimization objective for this conditional generative model is an importance-sampled estimate of the likelihood objective, Eh∼pθ(hi|xupper)  m Pm i=1 log pθ(xlower|hi)\n",
    "\n",
    "*  where m = 1 is used for training and m = 1000 is used for evaluation.\n",
    "\n",
    "* For bernoulli variables they use signmoid activation\n",
    "* For categorical variables they use \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmaxStructuredOutputPrediciton(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Predicts lower half of an mnist image given the top half.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GumbelSoftmaxStructuredOutputPrediciton, self).__init__()\n",
    "        self.setup_model()\n",
    "    \n",
    "    def setup_model(self):\n",
    "        self.input_layer = tf.keras.layers.Dense(200, activation=tf.nn.sigmoid) # [bs,392] => [bs,200]\n",
    "        self.categorical_layer = tf.keras.layers.Dense(200, activation=None) # [bs,200] => [bs,200]\n",
    "        self.output_layer = tf.keras.layers.Dense(392, activation=tf.nn.sigmoid) # [bs,200] => [bs,392]\n",
    "              \n",
    "        \n",
    "    def call(self, upper_image_half, temperature=0.5):\n",
    "        h1 = self.input_layer(upper_image_half)\n",
    "        logits = self.categorical_layer(h1) \n",
    "        h2 = gumbel_softmax_sample(logits, temperature)\n",
    "        lower_image_half = self.output_layer(h2)       \n",
    "        return lower_image_half\n",
    "\n",
    "sop_model = GumbelSoftmaxStructuredOutputPrediciton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9, nesterov=False, name='SGD') # m {3e−5, 1e−5, 3e−4, 1e−4, 3e−3, 1e−3};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* They use negative log likelihood from a bernoulli distribution where the probability is a sigmoid.\n",
    "\n",
    "* log likelihodd\n",
    "\n",
    "\n",
    "tf.nn.sigmoid_cross_entropy_with_logits \n",
    "-tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a75cbd4599ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m loss_object = tf.keras.losses.MeanSquaredError(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReductionV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses_utils' is not defined"
     ]
    }
   ],
   "source": [
    "loss_object =  tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=True, label_smoothing=0, name='binary_crossentropy'\n",
    ") \n",
    "\n",
    "loss_object = tf.keras.losses.MeanSquaredError(\n",
    "    name='mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loss, test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')   \n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='binary_accuracy', dtype=None, threshold=0.5)\n",
    "\n",
    "#test_loss =  tf.keras.metrics.BinaryCrossentropy( name='test_lsss', dtype=None, from_logits=False, label_smoothing=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images_upper_half, images_lower_half):\n",
    "    #images_lower_half = tf.cast(x=images_lower_half, dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted_lower_half = sop_model(images_upper_half, training=True)\n",
    "        #loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        #   labels=images_lower_half, \n",
    "        #   logits=predicted_lower_half, \n",
    "        #    name=None\n",
    "        #)\n",
    "        \n",
    "        loss =  loss_object(images_lower_half, predicted_lower_half)\n",
    "    gradients = tape.gradient(loss, sop_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, sop_model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(images_lower_half,predicted_lower_half)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images_arr,num_images=10):\n",
    "    num_imgages = images_arr.shape[0]\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_labels_batch = np.array(list(labeled_train_ds.take(1).as_numpy_iterator())[0])\n",
    "\n",
    "num_images = 10\n",
    "images = images_labels_batch[0][0:num_images].reshape([num_images, -1, 28]) \n",
    "labels = images_labels_batch[1][0:num_images].reshape([num_images, -1, 28 ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotImages(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotImages(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    #test_loss.reset_states()\n",
    "\n",
    "    # plot a few images every now and then\n",
    "    if epoch%10==0:\n",
    "        images_labels = np.array(list(labeled_train_ds.take(1).as_numpy_iterator())[0])\n",
    "        images, labels = images_labels\n",
    "        predictions = sop_model( images ,  training=False)\n",
    "\n",
    "        num_images = 10\n",
    "        images = images_labels_batch[0][0:num_images].reshape([num_images, -1, 28]) \n",
    "        labels = images_labels_batch[1][0:num_images].reshape([num_images, -1, 28 ]) \n",
    "        predictions = predictions.numpy()[0:num_images].reshape([num_images, -1, 28 ]) \n",
    "\n",
    "        plotImages(images)\n",
    "        plotImages(labels)\n",
    "        plotImages(predictions)\n",
    "\n",
    "\n",
    "\n",
    "    for images, labels in labeled_train_ds:         \n",
    "        train_step(images, labels)\n",
    "\n",
    "    #for test_images, test_labels in test_ds:\n",
    "    #    test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result() * 100))#,\n",
    "    #                    test_loss.result(),\n",
    "     #                   test_accuracy.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
