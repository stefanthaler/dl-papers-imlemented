{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel Softmax\n",
    "\n",
    "\n",
    "* Pre-print, published in ICLR 2017 https://arxiv.org/pdf/1611.01144.pdf\n",
    "\n",
    "* https://arxiv.org/pdf/1406.2989.pdf\n",
    "\n",
    "\n",
    "## Experiments\n",
    "\n",
    "* **Dataset**: We use the MNIST dataset with fixed binarization for training and evaluation\n",
    "* **Tricks**: We also found that variance normalization was necessary\n",
    "* **Network**: We used sigmoid activation functions for binary (Bernoulli) neural networks and softmax activations for categorical variables.\n",
    "* **Training**: Models were trained using stochastic gradient descent with momentum 0.9.\n",
    "* **Learning rates**:  are chosen from {3e−5, 1e−5, 3e−4, 1e−4, 3e−3, 1e−3}; we select the best learning rate for each estimator using the MNIST validation set, and report performance on the test set.\n",
    "* **Tasks** Each estimator is evaluated on two tasks: (1) structured output prediction and (2) variational training of generative models. \n",
    "\n",
    "### 1) Structured output prediction with stochastic binary networks\n",
    "\n",
    "### 2) Generative modelling with variational Autoencoders\n",
    "\n",
    "### 3) Generative semi supervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # do we need that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "mnist_data = tfds.load(\"binarized_mnist\", data_dir=\"/tf/data\")\n",
    "mnist_train, mnist_test = mnist_data[\"train\"], mnist_data[\"test\"]\n",
    "assert isinstance(mnist_train, tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split upper half / lower half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: {image: (28, 28, 1)}, types: {image: tf.uint8}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lower_upper_half(image):\n",
    "    print(image[\"image\"])\n",
    "    flat_image = tf.reshape(image[\"image\"], [-1], name=None)\n",
    "    upper_half, lower_half = tf.split(flat_image, num_or_size_splits=2, axis=0, num=None, name='split_image_to_upper_lower_half')\n",
    "\n",
    "    \n",
    "    return upper_half, lower_half#upper_half, lower_half # x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(28, 28, 1), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((392,), (392,)), types: (tf.uint8, tf.uint8)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_train_ds = mnist_train.map(split_lower_upper_half, num_parallel_calls=AUTOTUNE)\n",
    "labeled_train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_ds = labeled_train_ds.batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel Softmax \n",
    "* https://gist.github.com/ericjang/1001afd374c2c3b7752545ce6d9ed349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20): \n",
    "    \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "    U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "    return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature): \n",
    "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "    y = logits + sample_gumbel(tf.shape(logits))\n",
    "    return tf.nn.softmax( y / temperature)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "    Args:\n",
    "        logits: [batch_size, n_class] unnormalized log-probs\n",
    "        temperature: non-negative scalar\n",
    "        hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "    Returns:\n",
    "        [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "        If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "        be a probabilitiy distribution that sums to 1 across classes\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    if hard:\n",
    "        k = tf.shape(logits)[-1]\n",
    "        #y_hard = tf.cast(tf.one_hot(tf.argmax(y,1),k), y.dtype)\n",
    "        y_hard = tf.cast(tf.equal(y,tf.reduce_max(y,1,keep_dims=True)),y.dtype)\n",
    "        y = tf.stop_gradient(y_hard - y) + y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Structured output prediction with stochastic binary networks\n",
    "\n",
    "* **Task**: Predict lower half of mnist image given top half.\n",
    "* The minimization objective for this conditional generative model is an importance-sampled estimate of the likelihood objective, Eh∼pθ(hi|xupper)  m Pm i=1 log pθ(xlower|hi)\n",
    "\n",
    "*  where m = 1 is used for training and m = 1000 is used for evaluation.\n",
    "\n",
    "* For bernoulli variables they use signmoid activation\n",
    "* For categorical variables they use \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmaxStructuredOutputPrediciton(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Predicts lower half of an mnist image given the top half.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GumbelSoftmaxStructuredOutputPrediciton, self).__init__()\n",
    "        self.setup_model()\n",
    "    \n",
    "    def setup_model(self):\n",
    "        self.input_layer = tf.keras.layers.Dense(200, activation=tf.nn.sigmoid) # [bs,392] => [bs,200]\n",
    "        self.categorical_layer = tf.keras.layers.Dense(200, activation=None) # [bs,200] => [bs,200]\n",
    "        self.output_layer = tf.keras.layers.Dense(392, activation=tf.nn.sigmoid) # [bs,200] => [bs,392]\n",
    "              \n",
    "        \n",
    "    def call(self, upper_image_half, temperature=1.0):\n",
    "        h1 = self.input_layer(upper_image_half)\n",
    "        logits = self.categorical_layer(h1) \n",
    "        h2 = gumbel_softmax_sample(logits, temperature)\n",
    "        lower_image_half = self.output_layer(h2)       \n",
    "        return lower_image_half\n",
    "\n",
    "sop_model = GumbelSoftmaxStructuredOutputPrediciton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=False, name='SGD') # m {3e−5, 1e−5, 3e−4, 1e−4, 3e−3, 1e−3};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* They use negative log likelihood from a bernoulli distribution where the probability is a sigmoid.\n",
    "\n",
    "* log likelihodd\n",
    "\n",
    "\n",
    "tf.nn.sigmoid_cross_entropy_with_logits \n",
    "-tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object =  tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False, label_smoothing=0, name='binary_crossentropy'\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loss, test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')   \n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='binary_accuracy', dtype=None, threshold=0.5)\n",
    "\n",
    "#test_loss =  tf.keras.metrics.BinaryCrossentropy( name='test_lsss', dtype=None, from_logits=False, label_smoothing=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images_upper_half, images_lower_half):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted_lower_half = sop_model(images_upper_half, training=True)\n",
    "        loss = loss_object(images_lower_half, predicted_lower_half)\n",
    "    gradients = tape.gradient(loss, sop_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, sop_model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(images_lower_half,predicted_lower_half)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images_arr):\n",
    "    num_imgages = images_arr.shape[0]\n",
    "    fig, axes = plt.subplots(1, num_imgages, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_labels_batch = np.array(list(labeled_train_ds.take(1).as_numpy_iterator())[0])\n",
    "\n",
    "num_images = 10\n",
    "images = images_labels_batch[0][0:num_images].reshape([num_images, -1, 28]) \n",
    "labels = images_labels_batch[1][0:num_images].reshape([num_images, -1, 28 ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAABQCAYAAACULhC6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAEzklEQVR4nO3d0XHUMBQF0N1MqqAKmmCogCqpgKEJqqAMzGcGj8kqd2X7yTrnNwkR1lvJuaPRuy/LcgMAAAAAgI96OXsAAAAAAACMScAMAAAAAEBEwAwAAAAAQETADAAAAABARMAMAAAAAEBEwAwAAAAAQOT1vS9+efm2HDUQxvDzz/d7y/epHdbUDim1Q0rtkFI7pNQOKbVDSu2QaqkddcPa/+rGCWYAAAAAACICZgAAAAAAIgJmAAAAAAAiAmYAAAAAACICZgAAAAAAIgJmAAAAAAAiAmYAAAAAACICZgAAAAAAIgJmAAAAAAAiAmYAAAAAACICZgAAAAAAIgJmAAAAAAAir2cPoKIfv389/J6vnz4fMBIAAKht693ZuzIAwDycYAYAAAAAICJgBgAAAAAgImAGAAAAACAiYAYAAAAAIKLJ34Z1U5KtxiUtjQC3/i0AqGq9t9nDAAAAeMQJZgAAAAAAIgJmAAAAAAAiAmYAAAAAACICZgAAAAAAIpr8NdhqcrTV5E8zJABG0dqsFmCtZf3wrsyWlmay6f6kvuqr8u6hVuCa7B/ncoIZAAAAAICIgBkAAAAAgIiAGQAAAACAiIAZAAAAAICIJn8daWYCc2ttKmBdAGbkPek61vNWpXEXtaTNIPf8fbebdYe+daeerkFNzM07ah9OMAMAAAAAEBEwAwAAAAAQETADAAAAABBxBzNAgzPuCFxzDxQp96NytNaac+ddfdaPuZhvjra15qtDejq6ntwHf56ePSLWP2u+HnOCGQAAAACAiIAZAAAAAICIgBkAAAAAgIiAGQAAAACAiCZ/O3MxOGvppf+aBVxby3xrhgW0OKORqIZMrNmf6vO55Wit60LP9WPvOvf3/nlGX8PUzlj8Lf6YE8wAAAAAAEQEzAAAAAAARATMAAAAAABEBMwAAAAAAEQ0+duZS79r2bvxUc9GA6M3Laiq6nNNx9Xz/2O9gvFUXdO2bK0xI41/Vi3zZv8Yk88kM2hZn9R9fSPNkT2xDvvcsZxgBgAAAAAgImAGAAAAACAiYAYAAAAAICJgBgAAAAAgoskflzVrw72tsWo08Gb9LEaaW2jxTE1bK66rwtzan8Zkn5xLz6aOFWrHGkOLkWqaj0vXAfN9TRr/7ccJZgAAAAAAIgJmAAAAAAAiAmYAAAAAACICZgAAAAAAIpr8daSJBBWow/f1vMB/z2et0QB7s1aMaeS1Qc3Vl9ZX68+pgTG1zNvea5PaYU+a0NaTrinmjUTa+M/a8S8nmAEAAAAAiAiYAQAAAACICJgBAAAAAIi4g7kj96/Ul96ts7eq45pVhfl4Zgw970oc5c5q3lSoX/qocP+g2qGnlnqyV4zJ3kNV7pa/rgrPvsIYeJ79qg8nmAEAAAAAiAiYAQAAAACICJgBAAAAAIgImAEAAAAAiGjy18CF32NqvXC/6sX863FpInmcqp/5nvPd8/NR9XnNynxcR4WmWRXGAOA9eC4t833GXqTm9tGzEWM6R5o6zs37bh9OMAMAAAAAEBEwAwAAAAAQETADAAAAABARMAMAAAAAELkvy3L2GAAAAAAAGJATzAAAAAAARATMAAAAAABEBMwAAAAAAEQEzAAAAAAARATMAAAAAABEBMwAAAAAAET+Ak9gd/Qxlf8JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotImages(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAABQCAYAAACULhC6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAFJElEQVR4nO3d0Y3jNhAG4N3DVZEq0kSQCq7KVBCkiVSRMuI8BnF8B/qXKA2H3/e69q4hjintD2Lm8/F4fAAAAAAAwLu+3P0BAAAAAABYk4AZAAAAAICIgBkAAAAAgIiAGQAAAACAiIAZAAAAAICIgBkAAAAAgMjXH/3wly/fHld9kEp+/+vPS//erz/9fOnfO+KPv3/7HHndrrVzppE6VDtznLkHrLRGqdHrlV6LlWrnDt32ijN1rJ10f3pVA8+/a+Q1V6hQrx1r59mRtR1ZozNrdSU71M4RM/cUtbOmO+4zz9TOva6+X/hf718jtVO1bq42+7lpJd+rGyeYAQAAAACICJgBAAAAAIgImAEAAAAAiPywB/MOZvffGfn9s/uXUl+F3mOQqtK3dQfpdX31PveU+s78Hp35PEIPVfZue9FeZvbvdq/jFTWA5xsS6uZ9TjADAAAAABARMAMAAAAAEBEwAwAAAAAQETADAAAAABDZashfhcElBvqRUhNr6vadN+zgOrOH0FJflSFsz0Y+l5qrr0ItfXyoHf5vtAaq1DC1Gf7Ylz2AVezyrOMEMwAAAAAAEQEzAAAAAAARATMAAAAAAJG2PZhn9+NJe6Z07bUCndzR97RbX6bVP//KXPvezlzfdF/Tz3Iv1haYKd1jus05AfraZR9yghkAAAAAgIiAGQAAAACAiIAZAAAAAICIgBkAAAAAgEjbIX8Asz036589CHC2Mz+/IWDXcV25mprbi/2clalfRp5v1cS9zhywPntYu1rZR7f/9a/gBDMAAAAAABEBMwAAAAAAEQEzAAAAAAARATMAAAAAAJG2Q/5mN4qHVIXm8AaevG/k+py571Sok1Fq57iV1pu9+H4zuwbUGDOprz4MatvLmWtkWBt3e665rnuQE8wAAAAAAEQEzAAAAAAARATMAAAAAABEBMwAAAAAAETaDvnTuJ0KjtThzMbvXZvKV3Tm4L8K1M45Vq4B9mIobF+j9yf7VQ+j61jh+63m+hq9pxjoB9zNPvQ+J5gBAAAAAIgImAEAAAAAiAiYAQAAAACItO3BfMQu/VGAe4z0mhvdh9LeUPa5NVk3YGX2sPpGniuOPKM8v1e/5T7StdTnlKt1m5HDHO5X73OCGQAAAACAiIAZAAAAAICIgBkAAAAAgIiAGQAAAACASJshfwZdUYHG76TSvcge1pe1ZbaR4SXqkJTaqeXMoVZHnnc9K9dSdT3sH1SVDmZnfUfuo7vUjRPMAAAAAABEBMwAAAAAAEQEzAAAAAAARATMAAAAAABE2gz569okm7WMDEwaeR8AVGDw316sLStTv/VYEyo6MtxSTcP3OcEMAAAAAEBEwAwAAAAAQETADAAAAABARMAMAAAAAECkzZA/AFiFASFUcGTIDbC+0XtRhb3CfbM+a8QqXtXq6D73/Dp1T6LrEG0nmAEAAAAAiAiYAQAAAACICJgBAAAAAIgImAEAAAAAiBjyBycaGQ7QoXk7AOsbuR+9uq91HUwCvOb73Ze1BcjZQ//LCWYAAAAAACICZgAAAAAAIgJmAAAAAAAiejADAPCS3nIAQHeed+A4J5gBAAAAAIgImAEAAAAAiAiYAQAAAACICJgBAAAAAIgImAEAAAAAiAiYAQAAAACICJgBAAAAAIgImAEAAAAAiAiYAQAAAACIfD4ej7s/AwAAAAAAC3KCGQAAAACAiIAZAAAAAICIgBkAAAAAgIiAGQAAAACAiIAZAAAAAICIgBkAAAAAgMg/nmyBFu0V3xEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotImages(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def plot_batch(batch):\n",
    "   \n",
    "    retu\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TakeDataset' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ecf01b97d35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# plot one batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#plot_batch(labeled_train_ds.take(1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msop_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_train_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TakeDataset' object does not support indexing"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    #test_loss.reset_states()\n",
    "\n",
    "    # plot one batch\n",
    "    #plot_batch(labeled_train_ds.take(1))\n",
    "    predictions = sop_model( training=False)\n",
    "    \n",
    "\n",
    "    for images, labels in labeled_train_ds:         \n",
    "        train_step(images, labels)\n",
    "\n",
    "    #for test_images, test_labels in test_ds:\n",
    "    #    test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result() * 100))#,\n",
    "    #                    test_loss.result(),\n",
    "     #                   test_accuracy.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
