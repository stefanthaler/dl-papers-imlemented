{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel Softmax\n",
    "\n",
    "\n",
    "* Pre-print, published in ICLR 2017 https://arxiv.org/pdf/1611.01144.pdf\n",
    "\n",
    "* https://arxiv.org/pdf/1406.2989.pdf\n",
    "\n",
    "\n",
    "## Experiments\n",
    "\n",
    "* **Dataset**: We use the MNIST dataset with fixed binarization for training and evaluation\n",
    "* **Tricks**: We also found that variance normalization was necessary\n",
    "* **Network**: We used sigmoid activation functions for binary (Bernoulli) neural networks and softmax activations for categorical variables.\n",
    "* **Training**: Models were trained using stochastic gradient descent with momentum 0.9.\n",
    "* **Learning rates**:  are chosen from {3e−5, 1e−5, 3e−4, 1e−4, 3e−3, 1e−3}; we select the best learning rate for each estimator using the MNIST validation set, and report performance on the test set.\n",
    "* **Tasks** Each estimator is evaluated on two tasks: (1) structured output prediction and (2) variational training of generative models. \n",
    "\n",
    "### 1) Structured output prediction with stochastic binary networks\n",
    "\n",
    "### 2) Generative modelling with variational Autoencoders\n",
    "\n",
    "### 3) Generative semi supervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # do we need that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "mnist_data = tfds.load(\"binarized_mnist\", data_dir=\"/tf/data\")\n",
    "mnist_train, mnist_test = mnist_data[\"train\"], mnist_data[\"test\"]\n",
    "assert isinstance(mnist_train, tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split upper half / lower half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: {image: (28, 28, 1)}, types: {image: tf.uint8}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lower_upper_half(image):\n",
    "    print(image[\"image\"])\n",
    "    flat_image = tf.reshape(image[\"image\"], [-1], name=None)\n",
    "    upper_half, lower_half = tf.split(flat_image, num_or_size_splits=2, axis=0, num=None, name='split_image_to_upper_lower_half')\n",
    "\n",
    "    \n",
    "    return upper_half, lower_half#upper_half, lower_half # x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(28, 28, 1), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((392,), (392,)), types: (tf.uint8, tf.uint8)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_train_ds = mnist_train.map(split_lower_upper_half, num_parallel_calls=AUTOTUNE)\n",
    "labeled_train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_ds = labeled_train_ds.batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel Softmax \n",
    "* https://gist.github.com/ericjang/1001afd374c2c3b7752545ce6d9ed349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20): \n",
    "    \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "    U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "    return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature): \n",
    "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "    y = logits + sample_gumbel(tf.shape(logits))\n",
    "    return tf.nn.softmax( y / temperature)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "    Args:\n",
    "        logits: [batch_size, n_class] unnormalized log-probs\n",
    "        temperature: non-negative scalar\n",
    "        hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "    Returns:\n",
    "        [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "        If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "        be a probabilitiy distribution that sums to 1 across classes\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    if hard:\n",
    "        k = tf.shape(logits)[-1]\n",
    "        #y_hard = tf.cast(tf.one_hot(tf.argmax(y,1),k), y.dtype)\n",
    "        y_hard = tf.cast(tf.equal(y,tf.reduce_max(y,1,keep_dims=True)),y.dtype)\n",
    "        y = tf.stop_gradient(y_hard - y) + y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Structured output prediction with stochastic binary networks\n",
    "\n",
    "* **Task**: Predict lower half of mnist image given top half.\n",
    "* The minimization objective for this conditional generative model is an importance-sampled estimate of the likelihood objective, Eh∼pθ(hi|xupper)  m Pm i=1 log pθ(xlower|hi)\n",
    "\n",
    "*  where m = 1 is used for training and m = 1000 is used for evaluation.\n",
    "\n",
    "* For bernoulli variables they use signmoid activation\n",
    "* For categorical variables they use \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmaxStructuredOutputPrediciton(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Predicts lower half of an mnist image given the top half.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GumbelSoftmaxStructuredOutputPrediciton, self).__init__()\n",
    "        self.setup_model()\n",
    "    \n",
    "    def setup_model(self):\n",
    "        self.input_layer = tf.keras.layers.Dense(200, activation=tf.nn.sigmoid) # [bs,392] => [bs,200]\n",
    "        self.categorical_layer = tf.keras.layers.Dense(200, activation=None) # [bs,200] => [bs,200]\n",
    "        self.output_layer = tf.keras.layers.Dense(392, activation=tf.nn.sigmoid) # [bs,200] => [bs,392]\n",
    "              \n",
    "        \n",
    "    def call(self, upper_image_half, temperature=1.0):\n",
    "        h1 = self.input_layer(upper_image_half)\n",
    "        logits = self.categorical_layer(h1) \n",
    "        h2 = gumbel_softmax_sample(logits, temperature)\n",
    "        lower_image_half = self.output_layer(h2)       \n",
    "        return lower_image_half\n",
    "\n",
    "sop_model = GumbelSoftmaxStructuredOutputPrediciton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=False, name='SGD') # m {3e−5, 1e−5, 3e−4, 1e−4, 3e−3, 1e−3};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* They use negative log likelihood from a bernoulli distribution where the probability is a sigmoid.\n",
    "\n",
    "* log likelihodd\n",
    "\n",
    "\n",
    "tf.nn.sigmoid_cross_entropy_with_logits \n",
    "-tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object =  tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False, label_smoothing=0, name='binary_crossentropy'\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loss, test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')   \n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='binary_accuracy', dtype=None, threshold=0.5)\n",
    "\n",
    "#test_loss =  tf.keras.metrics.BinaryCrossentropy( name='test_lsss', dtype=None, from_logits=False, label_smoothing=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images_upper_half, images_lower_half):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted_lower_half = sop_model(images_upper_half, training=True)\n",
    "        loss = loss_object(images_lower_half, predicted_lower_half)\n",
    "    gradients = tape.gradient(loss, sop_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, sop_model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(images_lower_half,predicted_lower_half)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5715445876121521, Accuracy: 86.0301284790039\n",
      "Epoch 2, Loss: 0.5591833591461182, Accuracy: 86.05609893798828\n",
      "Epoch 3, Loss: 0.5475417971611023, Accuracy: 86.07710266113281\n",
      "Epoch 4, Loss: 0.5365837812423706, Accuracy: 86.09388732910156\n",
      "Epoch 5, Loss: 0.526249885559082, Accuracy: 86.10809326171875\n",
      "Epoch 6, Loss: 0.5164836645126343, Accuracy: 86.1202392578125\n",
      "Epoch 7, Loss: 0.5072522759437561, Accuracy: 86.13046264648438\n",
      "Epoch 8, Loss: 0.49843671917915344, Accuracy: 86.13961029052734\n",
      "Epoch 9, Loss: 0.4897180199623108, Accuracy: 86.14728546142578\n",
      "Epoch 10, Loss: 0.4788978397846222, Accuracy: 86.15409088134766\n",
      "Epoch 11, Loss: 0.46499863266944885, Accuracy: 86.15968322753906\n",
      "Epoch 12, Loss: 0.4525068402290344, Accuracy: 86.16505432128906\n",
      "Epoch 13, Loss: 0.4413733184337616, Accuracy: 86.16986083984375\n",
      "Epoch 14, Loss: 0.4313271641731262, Accuracy: 86.17449951171875\n",
      "Epoch 15, Loss: 0.4222269356250763, Accuracy: 86.17874908447266\n",
      "Epoch 16, Loss: 0.4139557480812073, Accuracy: 86.18260955810547\n",
      "Epoch 17, Loss: 0.4064217805862427, Accuracy: 86.1861343383789\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "\n",
    "    for images, labels in labeled_train_ds:        \n",
    "        train_step(images, labels)\n",
    "\n",
    "    #for test_images, test_labels in test_ds:\n",
    "    #    test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result() * 100))#,\n",
    "    #                    test_loss.result(),\n",
    "     #                   test_accuracy.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
